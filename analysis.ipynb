{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import sklearn.tree as tree\n",
    "\n",
    "# # Load the decision tree model\n",
    "# with open(os.path.join(\"dt_model\", \"decision_tree_depth_8.pkl\"), \"rb\") as f:\n",
    "#     dt_model = pickle.load(f)\n",
    "\n",
    "# # Print depth\n",
    "# print(f\"Decision Tree Depth: {dt_model.get_depth()}\")\n",
    "\n",
    "# # Load the saved feature names\n",
    "# with open(os.path.join(\"dt_model\", \"feature_names.pkl\"), \"rb\") as f:\n",
    "#     feature_names = pickle.load(f)\n",
    "\n",
    "# # Verify dimensions match\n",
    "# if dt_model.n_features_in_ == len(feature_names):\n",
    "#     print(f\"Found {len(feature_names)} feature names, matching model dimensions\")\n",
    "# else:\n",
    "#     print(f\"Warning: Model has {dt_model.n_features_in_} features but found {len(feature_names)} feature names\")\n",
    "#     # Fall back to generic names if needed\n",
    "#     feature_names = [f\"feature_{i}\" for i in range(dt_model.n_features_in_)]\n",
    "\n",
    "# # Plot the tree\n",
    "# plt.figure(figsize=(20, 15))\n",
    "# tree.plot_tree(dt_model, \n",
    "#               feature_names=feature_names,\n",
    "#               class_names=['action_0', 'action_1'],\n",
    "#               filled=True,\n",
    "#               max_depth=None)\n",
    "# #plt.savefig(\"decision_tree_viz.png\", dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PCAP: tmp/20250328_162243/background_server_10.0.2.2_12345.pcap\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m exp_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20250328_162243\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     75\u001b[0m pcap_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp\u001b[39m\u001b[38;5;124m'\u001b[39m, exp_id, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground_server_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver_ip\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver_port\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pcap\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m ooo_packets_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_out_of_order_packets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcap_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_ip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Save results to CSV\u001b[39;00m\n\u001b[1;32m     80\u001b[0m ooo_packets_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_of_order_packets.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mget_out_of_order_packets\u001b[0;34m(pcap_file, dst_ip, dst_port)\u001b[0m\n\u001b[1;32m     25\u001b[0m flows_seq \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     26\u001b[0m out_of_order_pkts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pkt \u001b[38;5;129;01min\u001b[39;00m capture:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m         tcp \u001b[38;5;241m=\u001b[39m pkt\u001b[38;5;241m.\u001b[39mtcp\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyshark/capture/capture.py:212\u001b[0m, in \u001b[0;36mCapture._packets_from_tshark_sync\u001b[0;34m(self, packet_count, existing_process)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a generator of packets.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03mThis is the sync version of packets_from_tshark. It wait for the completion of each coroutine and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m:param packet_count: If given, stops after this amount of packets is captured.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# NOTE: This has code duplication with the async version, think about how to solve this\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m tshark_process \u001b[38;5;241m=\u001b[39m existing_process \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meventloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tshark_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_tshark_output_parser()\n\u001b[1;32m    215\u001b[0m packets_captured \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/base_events.py:592\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 592\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[1;32m    595\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/base_events.py:552\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    555\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "import pyshark\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "def get_out_of_order_packets(pcap_file, dst_ip, dst_port):\n",
    "    \"\"\"\n",
    "    Extract out-of-order TCP packets received by the server from pcap.\n",
    "\n",
    "    Args:\n",
    "        pcap_file (str): Path to server-side pcap file.\n",
    "        dst_ip (str): IP address of the receiving server.\n",
    "        dst_port (int or str): Port number of the server.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing details of out-of-order packets.\n",
    "    \"\"\"\n",
    "    print(f\"Processing PCAP: {pcap_file}\")\n",
    "    capture = pyshark.FileCapture(\n",
    "        pcap_file,\n",
    "        display_filter=f'tcp.dstport=={dst_port} && ip.dst=={dst_ip}',\n",
    "        use_json=True\n",
    "    )\n",
    "\n",
    "    # Data structure to track highest seen sequence number per flow\n",
    "    flows_seq = {}\n",
    "    out_of_order_pkts = []\n",
    "\n",
    "    for pkt in capture:\n",
    "        try:\n",
    "            tcp = pkt.tcp\n",
    "            ip = pkt.ip\n",
    "\n",
    "            flow_key = (\n",
    "                ip.src, int(tcp.srcport), ip.dst, int(tcp.dstport)\n",
    "            )\n",
    "\n",
    "            seq_num = int(tcp.seq)\n",
    "            seq_len = int(tcp.len)  # TCP payload length\n",
    "\n",
    "            if flow_key not in flows_seq:\n",
    "                flows_seq[flow_key] = seq_num + seq_len\n",
    "                continue\n",
    "\n",
    "            expected_seq = flows_seq[flow_key]\n",
    "\n",
    "            if seq_num < expected_seq:\n",
    "                # Out-of-order packet detected\n",
    "                out_of_order_pkts.append({\n",
    "                    'timestamp': pkt.sniff_time,\n",
    "                    'src_ip': ip.src,\n",
    "                    'src_port': int(tcp.srcport),\n",
    "                    'dst_ip': ip.dst,\n",
    "                    'dst_port': int(tcp.dstport),\n",
    "                    'sequence_number': seq_num,\n",
    "                    'expected_sequence_number': expected_seq,\n",
    "                    'payload_length': seq_len\n",
    "                })\n",
    "            else:\n",
    "                # Update highest sequence seen\n",
    "                flows_seq[flow_key] = seq_num + seq_len\n",
    "\n",
    "        except AttributeError:\n",
    "            # Skip non-TCP packets or incomplete packets\n",
    "            continue\n",
    "\n",
    "    capture.close()\n",
    "\n",
    "    print(f\"Total out-of-order packets found: {len(out_of_order_pkts)}\")\n",
    "    return pd.DataFrame(out_of_order_pkts)\n",
    "\n",
    "\n",
    "server_ip = '10.0.2.2'  \n",
    "server_port = 12345 \n",
    "exp_id = '20250328_162243'\n",
    "pcap_file = os.path.join('tmp', exp_id, f'background_server_{server_ip}_{server_port}.pcap')\n",
    "\n",
    "ooo_packets_df = get_out_of_order_packets(pcap_file, server_ip, server_port)\n",
    "\n",
    "# Save results to CSV\n",
    "ooo_packets_df.to_csv('out_of_order_packets.csv', index=False)\n",
    "\n",
    "print(ooo_packets_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
